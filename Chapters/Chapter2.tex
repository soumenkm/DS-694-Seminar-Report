\chapter{Background and Foundations}\label{chp:Background}
\section{Overview of Language Models}
\subsection{Evolution of Language Models}
Initially, LMs were based on n-gram models and simple neural networks, which were limited by their inability to capture long-range dependencies in text. The introduction of recurrent neural networks (RNNs), particularly Long Short-Term Memory (LSTM) networks by \citet{hochreiter1997long}, marked a significant advancement, enabling models to remember information over extended sequences. The shift towards attention-based models, particularly the Transformer architecture, has set the current state-of-the-art, enabling the training of large-scale language models like GPT (Generative Pre-trained Transformer) by \citet{radford2018improving} and BERT (Bidirectional Encoder Representations from Transformers) by \citet{devlin2018bert}, which leverage vast amounts of data and computational power to achieve improved performance across a range of language tasks.

\subsection{Attention Mechanism}
The attention mechanism improved language modeling by allowing models to focus on different parts of the input sequence when predicting an output, improving the context sensitivity of the model outputs. Introduced by \citet{bahdanau2014neural} in the context of neural machine translation, the mechanism addresses the limitations of earlier sequence-to-sequence models by selecting a subset of the input tokens to pay attention to. This concept was further extended by \citet{vaswani2017attention} through the Transformer model, which employs self-attention to weigh the significance of all tokens in the input sequence relative to each other, leading to significant improvements in processing speed and model performance.

\section{Development of Multilingual Models}
\subsection{mBERT}
Multilingual BERT (mBERT) by \citet{devlin2018bert} is one of the first large-scale multilingual models that has achieved notable success in cross-lingual understanding. mBERT is a variant of the original BERT model that was trained on the concatenated text corpora of 104 languages from Wikipedia. mBERT utilizes the same model architecture as the original BERT but extends its application to multiple languages without modifying the underlying transformer architecture. Despite not being explicitly trained to model cross-lingual relationships, mBERT has shown remarkable zero-shot learning capabilities, where a model trained on data in one language performs well on the same task in other languages. This phenomenon is attributed to the shared subword vocabulary and the model's ability to learn language-agnostic representations during training.

\subsection{XLM-R}
Expanding on the idea of multilingual language modeling, \citet{conneau2020unsupervised} introduced XLM-R (Cross-lingual Language Model-RoBERTa), which builds upon the RoBERTa model by \citet{liu2019roberta}, an optimized version of BERT that modifies key hyperparameters. XLM-R was trained on 2.5TB of filtered CommonCrawl data across 100 languages, making it one of the most comprehensive and diverse datasets used in multilingual model training to date. Unlike mBERT, XLM-R uses a more robust training regime and a larger, more diverse dataset, leading to significant improvements in multilingual performance, particularly in low-resource languages. XLM-R has demonstrated state-of-the-art performance on a variety of cross-lingual benchmarks, highlighting its effectiveness in handling diverse linguistic features and its robustness in language transfer tasks.
