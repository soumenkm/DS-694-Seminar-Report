\section*{\centering \textbf{\LARGE Abstract}}
\paragraph{} This report explores the field of cross-lingual knowledge transfer within multilingual language models, highlighting advancements and challenges in the domain. We begin by defining the problem of cross-lingual knowledge transfer and discuss the motivation behind this research. We delve into various fine-tuning techniques that enhance the performance of these models on specific tasks and languages without compromising their general applicability. These techniques include generalizable fine-tuning, composable sparse fine-tuning, and adapter-based frameworks like MAD-X. The discussion extends to how facts are represented within these models, with a focus on task-specific model editing and the geometry of multilingual representations. Furthermore, the report examines mechanisms for cross-lingual transfer, emphasizing the role of knowledge editing and cross lingual entity alignment that support transfer learning. The conclusion summarizes key findings and proposes future research directions aimed at overcoming current limitations and enhancing the efficacy of cross-lingual knowledge transfer. 









