@article{bahdanau2014neural,
	title={Neural machine translation by jointly learning to align and translate},
	author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	journal={arXiv preprint arXiv:1409.0473},
	year={2014}
}

@article{vaswani2017attention,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}

@article{hochreiter1997long,
	title={Long short-term memory},
	author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	journal={Neural computation},
	volume={9},
	number={8},
	pages={1735--1780},
	year={1997},
	publisher={MIT Press}
}

@article{devlin2018bert,
	title={BERT: Pre-training of deep bidirectional transformers for language understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	journal={arXiv preprint arXiv:1810.04805},
	year={2018}
}

@article{radford2018improving,
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	biburl = {https://www.bibsonomy.org/bibtex/273ced32c0d4588eb95b6986dc2c8147c/jonaskaiser},
	interhash = {5c343ed9a31ac52fd17a898f72af228f},
	intrahash = {73ced32c0d4588eb95b6986dc2c8147c},
	keywords = {},
	timestamp = {2020-07-14T16:49:49.000+0200},
	title = {Improving language understanding by generative pre-training},
	year = 2018
}

@article{liu2019roberta,
	title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
	author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
	year={2019},
	eprint={1907.11692},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{conneau2020unsupervised,
	title={Unsupervised Cross-lingual Representation Learning at Scale}, 
	author={Alexis Conneau and Kartikay Khandelwal and Naman Goyal and Vishrav Chaudhary and Guillaume Wenzek and Francisco Guzmán and Edouard Grave and Myle Ott and Luke Zettlemoyer and Veselin Stoyanov},
	year={2020},
	eprint={1911.02116},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@inproceedings{houlsby2019parameter,
	title={Parameter-efficient transfer learning for NLP},
	author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
	booktitle={Proceedings of the 36th International Conference on Machine Learning},
	pages={2790--2799},
	year={2019}
}

@inproceedings{pfeiffer2020mad,
	title={MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer},
	author={Pfeiffer, Jonas and Vuli\'c, Ivan and Gurevych, Iryna and Ruder, Sebastian},
	booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	year={2020}
}

@article{xu2021raise,
	title={Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning}, 
	author={Runxin Xu and Fuli Luo and Zhiyuan Zhang and Chuanqi Tan and Baobao Chang and Songfang Huang and Fei Huang},
	year={2021},
	eprint={2109.05687},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{ansell2023composable,
	title={Composable Sparse Fine-Tuning for Cross-Lingual Transfer}, 
	author={Alan Ansell and Edoardo Maria Ponti and Anna Korhonen and Ivan Vulić},
	year={2023},
	eprint={2110.07560},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{frankle2019lottery,
	title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks}, 
	author={Jonathan Frankle and Michael Carbin},
	year={2019},
	eprint={1803.03635},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@misc{ilharco2023editing,
	title={Editing Models with Task Arithmetic}, 
	author={Gabriel Ilharco and Marco Tulio Ribeiro and Mitchell Wortsman and Suchin Gururangan and Ludwig Schmidt and Hannaneh Hajishirzi and Ali Farhadi},
	year={2023},
	eprint={2212.04089},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@article{zhao2024tracing,
	title={Tracing the Roots of Facts in Multilingual Language Models: Independent, Shared, and Transferred Knowledge}, 
	author={Xin Zhao and Naoki Yoshinaga and Daisuke Oba},
	year={2024},
	eprint={2403.05189},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{chang2022geometry,
	title={The Geometry of Multilingual Language Model Representations}, 
	author={Tyler A. Chang and Zhuowen Tu and Benjamin K. Bergen},
	year={2022},
	eprint={2205.10964},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{wang2023crosslingual,
	title={Cross-Lingual Knowledge Editing in Large Language Models}, 
	author={Jiaan Wang and Yunlong Liang and Zengkui Sun and Yuxuan Cao and Jiarong Xu},
	year={2023},
	eprint={2309.08952},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{jiang2023unsupervised,
	title={Unsupervised Deep Cross-Language Entity Alignment}, 
	author={Chuanyu Jiang and Yiming Qian and Lijun Chen and Yang Gu and Xia Xie},
	year={2023},
	eprint={2309.10598},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{guo2021parameterefficient,
	title={Parameter-Efficient Transfer Learning with Diff Pruning}, 
	author={Demi Guo and Alexander M. Rush and Yoon Kim},
	year={2021},
	eprint={2012.07463},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}